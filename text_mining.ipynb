{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import morfeusz2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator as op\n",
    "import itertools as it\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances, cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korpus dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dir = \"./Literatura - original\"\n",
    "corpus = PlaintextCorpusReader(corpus_dir, r'.*.txt')\n",
    "files_names = corpus.fileids()\n",
    "files_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wstepne przygotowanie dokumentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for file in files_names:\n",
    "    documents[file] = corpus.raw(file)\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist_file = open(\"./stopwords_pl.txt\", \"r\", encoding=\"UTF-8\")\n",
    "stoplist = stoplist_file.read().splitlines()\n",
    "stoplist_file.close()\n",
    "stoplist = stoplist[4:]\n",
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    morf = morfeusz2.Morfeusz()\n",
    "    segments = it.groupby(morf.analyse(text), op.itemgetter(0,1))\n",
    "    def disambiguate(group):\n",
    "        pairs = ((len(descr), lemma) \n",
    "                 for _, _, (_, lemma, descr, _, _, ) in group)\n",
    "        perpl, lemma = min(pairs)\n",
    "        return lemma.split(\":\")\n",
    "    lemmas = (disambiguate(group) for key, group in segments)\n",
    "    return \" \".join(filter(str.isalpha, lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in documents:\n",
    "    documents[key] = documents[key].lower()\n",
    "    documents[key] = \"\".join([char for char in documents[key] if char not in string.punctuation])\n",
    "    documents[key] = lemmatize(documents[key])\n",
    "    documents[key] = \" \".join([word for word in word_tokenize(documents[key]) if word not in stoplist])\n",
    "\n",
    "print(json.dumps(documents, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morf = morfeusz2.Morfeusz()\n",
    "morf.analyse(\"Ala ma kota\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie macierzy częstości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.DataFrame.from_dict(documents, orient=\"index\")\n",
    "docs.columns = ['content']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "counts_tf = count_vectorizer.fit_transform(docs['content'])\n",
    "counts_tf.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "counts_tfidf = tfidf_vectorizer.fit_transform(docs['content'])\n",
    "counts_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Katalogi na wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./wordclouds\"):\n",
    "    os.mkdir(\"./wordclouds\")\n",
    "if not os.path.exists(\"./topics\"):\n",
    "    os.mkdir(\"./topics\")\n",
    "if not os.path.exists(\"./clusters\"):\n",
    "    os.mkdir(\"./clusters\")\n",
    "if not os.path.exists(\"./ngrams\"):\n",
    "    os.mkdir(\"./ngrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chumry tagów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    background_color= \"white\",\n",
    "    max_words = 5000,\n",
    "    contour_width=3,\n",
    "    contour_color=\"steelblue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in docs.iterrows():\n",
    "    wordcloud.generate(row['content'])\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(index.replace(\".txt\", \"\"))\n",
    "    plt.savefig(\"./wordclouds/{}\".format(index.replace(\".txt\", \".png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza tematyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title, subplots):\n",
    "    colors = [\"forestgreen', 'lightskyblue', 'hotpink', 'turquoise', 'steelblue', 'crimson', 'seagreen', 'orange', 'purple', 'brown'\"]\n",
    "    fig, axes = plt.subplots(*subplots, figsize=(30,15), share=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7, color=colors[topic_idx])\n",
    "        ax.set_title(f\"Topic {topic_idx + 1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.savefig(\"./topics{}.png\".format(title.replace(\"\",\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_documents(model, count, n_components, title):\n",
    "    colors = [\"forestgreen', 'lightskyblue', 'hotpink', 'turquoise', 'steelblue', 'crimson', 'seagreen', 'orange', 'purple', 'brown'\"]\n",
    "    docs_topics = pd.DataFrame(model.transform(counts), columns-[f'Topic {i+1}' for i in range(n_components)])\n",
    "    docs_topics.index = [file_name.replace(\".txt\", \"\") for file_name in files_names]\n",
    "    plt.figure(figsize=(7,4))\n",
    "    left = [0] * len(docs_topics)\n",
    "    for i, col in enumerate(docs_topics.columns):\n",
    "        plt.barh(docs_topics.index, docs_topics[col], left=left, color=colors[i], label=col)\n",
    "        left = [left[j] + docs-topics[col].iloc[j] for j in range(len(docs_topics))]\n",
    "    plt.savefig(\"./topics/{}_docs.png\".format(title), bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 6\n",
    "n_top_words = 20 \n",
    "feaature_names = count_vectorizer.get_feature_names_out()\n",
    "subplots = (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=100,\n",
    "    learn_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(counts_tf)\n",
    "plot_top_words(lda, feature_names, n_top_words, \"Tematy w modelu LDA\", subplots)\n",
    "plot_documents(lda, counts_tf, n_topics, \"Tematy w modelu LDA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_fn = NMF(\n",
    "    n_components=n_topics,\n",
    "    random_state=1,\n",
    "    alpha_H=.00005,\n",
    "    alpha_W=.00005,\n",
    "    l1_ratio=.5\n",
    ")\n",
    "nmf_fn.fit(counts_tfidf)\n",
    "plot_top_words(nmf_fn, feature_names, n_top_words, \"Tematy w modelu NMF Norma macierzowa\", subplots)\n",
    "plot_documents(lda, counts_tf, n_topics, \"Tematy w modelu NMF Norma Macierzowa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_kl = NMF(\n",
    "    n_components=n_topics,\n",
    "    random_state=1,\n",
    "    beta_loss='kullback-leibler',\n",
    "    solver='mu',\n",
    "    max_iter=1000,\n",
    "    alpha_H=.00005,\n",
    "    alpha_W=.00005,\n",
    "    l1_ratio=.5\n",
    ")\n",
    "nmf_fn.fit(counts_tfidf)\n",
    "plot_top_words(nmf_fn, feature_names, n_top_words, \"Tematy w modelu NMF Kullback-Leibler\", subplots)\n",
    "plot_documents(lda, counts_tf, n_topics, \"Tematy w modelu NMF Kullback-Leibler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analiza skupień "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendogram(model, title, **kwargs):\n",
    "    counts = np.zeros(model.children_shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx in merge:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distance_, counts]\n",
    "    ).astype(float)\n",
    "    dendogram(linkage_matrix, **kwargs)\n",
    "    plt.title(title)\n",
    "    plt.savefig(\"./clusters/{}.png\".format(title.replace(\" \", \"_\")), bbox_inches=\"tight\")\n",
    "    plt.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = cosine_similarity(counts_tfidf, counts_tfidf).flatten().reshape(len(files_names), len(files_names))\n",
    "ed = euclidean_distances(counts_tf, counts_tf).flatten().reshape(len(files_names), len(files_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_ed_complete = AgglomerativeClustering(\n",
    "    n_clustering=3,\n",
    "    metric='precomputed',\n",
    "    linkage='complate',\n",
    "    compute_distances=True\n",
    ").fit(ed)\n",
    "plot_dendogram(\n",
    "    clustering_ed_complete,\n",
    "    title=\"Dendogram metryka euklidesowa, metoda pełnego wiązania\",\n",
    "    labels=[files_names.replace(\"txt\", \"\") for file_name in files_names],\n",
    "    orientation='left',\n",
    ")\n",
    "clustering_ed_complete.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_cs_ward = AgglomerativeClustering(\n",
    "    n_clustering=3,\n",
    "    metric='precomputed',\n",
    "    linkage='ward',\n",
    "    compute_distances=True\n",
    ").fit(cs)\n",
    "plot_dendogram(\n",
    "    clustering_cs_ward,\n",
    "    title=\"Dendogram metryka kątowa, metoda Warda\",\n",
    "    labels=[files_names.replace(\"txt\", \"\") for file_name in files_names],\n",
    "    orientation='left',\n",
    ")\n",
    "clustering_ed_complete.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_tokenized = {}\n",
    "for key in documents:\n",
    "    documents_tokenized[key] = word_tokenize(documents[key], language='polish')\n",
    "print(json.dumps(documents_tokenized, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1, 4):\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(30, 15))\n",
    "    axes = axes.flatten()\n",
    "    for i, key in enumerate(documents_tokenized):\n",
    "        if i < len(axes):\n",
    "            n_grams = pd.Series(ngrams(documents_tokenized[key], n)).value_counts()\n",
    "            n_grams[:5].plot.barh(ax=axes[i], title=\"{}-gramy w {}\".format(n, key.replace(\".txt\", \"\")))\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./ngrams/{}-grams.png\".format(n), bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \" \".join(docs['content']).split(\" \")\n",
    "n_grams = pd.Series(ngrams(texts, 3)).value_counts()\n",
    "ax = n_grams[:15].plot.barh()\n",
    "plt.gca().invert_yaxis()\n",
    "ax.bar_label(ax.containers[0], label_type='edge')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
